{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is comprehensive tutorial about Natural Language Processing for beginners and intermediate level users. The tutorial spans across 3 parts.\n",
    "[Part-1](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-1-ml-perspective/) discusses NLP with respect to traditional Machine Learning perspective. \n",
    "[Part 2](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-2-dl-perspective) explains NLP with respect to Deep Learning perspective and \n",
    "[Part-3](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-3-bert) demonstrates useful NLP state-of-art BERT embedding. I will be using [Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) dataset for our text modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"kk\" id=\"0.1\"></a>\n",
    "## Contents\n",
    "\n",
    "1. [NLP Introduction](#1)\n",
    "1. [Text Cleaning](#2)\n",
    "    1. [Text Standardization](#2.1)\n",
    "        1. [Convert to Lower Case](#2.1.1)\n",
    "        1. [Spelling Correction](#2.1.2)    \n",
    "    1.  [Eliminate Undesirable Items From Text](#2.2)\n",
    "         1. [Removing Additional Spaces](#2.2.1)\n",
    "         1. [Removing Punctuations](#2.2.2)\n",
    "         1. [Removing URLs](#2.2.3)\n",
    "         1. [Removing Digits](#2.2.4)\n",
    "         1. [Removing Stopwords](#2.2.5) \n",
    "    1. [Convert Non-Words to Words](#2.3)\n",
    "        1. [Convert Emoji Into Words](#2.3.1)  \n",
    "    1. [Convert Negative Word to its Antonyms](#2.4)\n",
    "    1. [Dealing With Base and Derived Words](#2.5)\n",
    "         1. [Stemming](#2.5.1)\n",
    "         1. [Lemmatization](#2.5.2)\n",
    "    1. [Extract Text Using BeautifulSoup](#2.6)    \n",
    "1. [Text to Numeric Conversion](#3)\n",
    "    1. [Pre Conversion](#3.1)\n",
    "        1. [Corpus](#3.1.1)\n",
    "        1. [Tokenization](#3.1.2)\n",
    "        1. [Bag of Words](#3.1.3)\n",
    "        1. [N-Gram](#3.1.4)\n",
    "    1.  [Conversion](#3.2)\n",
    "         1. [Count Vectorization](#3.2.1)\n",
    "         1. [TF-IDF Vectorization](#3.2.2)    \n",
    "    1. [Post Conversion Dimesionality Reduction](#3.3)\n",
    "        1. [SVD(TruncatedSVD)](#3.3.1)         \n",
    "1. [ML-Modeling](#4)\n",
    "    1. [Naive Bayes Classifer](#4.1)\n",
    "        1. [Gaussian Classifier](#4.1.1)\n",
    "        1. [Bernoulli  Classifier](#4.1.2)\n",
    "    1. [Logistic Regression](#4.2)\n",
    "    1. [SVM](#4.3)\n",
    "    1. [XGBoost](#4.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLP Introduction <a class=\"kk\" id=\"1\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "Natural Language processing enables computer to understand and process human languages which include both text and audio. In this tutorial we will look into text processing.  \n",
    "\n",
    "pre-requsite:  Python and its libraries.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Cleaning <a class=\"kk\" id=\"2\"></a>\n",
    "\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "Primary step of any Machine-Learning project is data cleaning. In text processing also, first we need to clean and standardise text. Lets look into some of the ways of cleaning textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Text Standardization <a class=\"kk\" id=\"2.1\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Convert to Lower Case <a class=\"kk\" id=\"2.1.1\"></a>\n",
    "\n",
    "Words with different cases are intercepted differently such as 'The' and 'the'. Hence all words should be converted into same case, preferably lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome to nlp tutorial\n"
     ]
    }
   ],
   "source": [
    "text =\"Welcome to NLP Tutorial\"\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Spelling Correction <a class=\"kk\" id=\"2.1.2\"></a>\n",
    "\n",
    "At times textual data such as social media data is prone to spelling errors. Spelling errors should be rectified early during the clean-up phase. Fortunately we have libraries available for spelling correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# !pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling correcting is properly performed\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "def correct_spellings(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_words.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "        \n",
    "\n",
    "\n",
    "text = \"Spelling correctin is proprly perfrmed\"\n",
    "text = correct_spellings(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Eliminate Undesirable Items From Text <a class=\"kk\" id=\"2.2\"></a>\n",
    "\n",
    "Texts contains many items that are not useful with respect to text processing and it is better to eliminate them before modelling. Lets look into items that should be considered for removal and repective codes to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1  Removing Additional Spaces <a class=\"kk\" id=\"2.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting double space text \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Correcting   double  space  text \"\n",
    "text = re.sub(' +', ' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Removing Punctuations <a class=\"kk\" id=\"2.2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence contains so many  punctuations\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"This! sentence, contains so: many - punctuations.\"\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Removing URLs  <a class=\"kk\" id=\"2.2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I search the answer in  ?\n"
     ]
    }
   ],
   "source": [
    "text = 'Shall I search the answer in www.google.com ?'\n",
    "text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4.  Removing Digits <a class=\"kk\" id=\"2.2.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Being no  team is more important or being no  but with fair play \n"
     ]
    }
   ],
   "source": [
    "text =\"Being no 1 team is more important or being no 3 but with fair play \"\n",
    "text= re.sub(r'[0-9]','',text)\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5. Removing Stopwords  <a class=\"kk\" id=\"2.2.5\"></a>\n",
    "\n",
    "Stopwords are the most common words in a language. For example 'is', 'the', 'that' etc. are stopwords in English language. Stopwords shall be removed during text clean-up phase. However removing stop word can change the meaning of sentence. For instance 'I didn't love politics' will get converted to 'I love politics' after removing stopword.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This important topic\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "text = \"This is not the most important topic\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # stop_words will contain  set all english stopwords\n",
    "    filtered_sentence = []   \n",
    "    for word in text.split(): \n",
    "        if word not in stop_words: \n",
    "            filtered_sentence.append(word) \n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "text = remove_stopwords(text)\n",
    "print(text) \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Convert Non-Words to Words <a class=\"kk\" id=\"2.3\"></a>\n",
    "\n",
    "Special symbols such as emoticon, emojis etc. are example of non-words.  These non-words should be either converted into words or removed from text. Emoji library converts emojis to equivalent words as shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Convert Emoji Into Words  <a class=\"kk\" id=\"2.3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :thumbs_up:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "text = 'Python is üëç'\n",
    "print(emoji.demojize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Convert Negative Word to its Antonyms <a class=\"kk\" id=\"2.4\"></a>\n",
    "\n",
    "Negative words should be replaced with their antonym for efficient processing. For instance 'not good‚Äô should be replaced with bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was unhappy with the score of team\n"
     ]
    }
   ],
   "source": [
    "text = \"He was not happy with the score of team\"\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "def convert_to_antonym(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = []\n",
    "    temp_word = ''\n",
    "    for word in words:\n",
    "        antonyms = []\n",
    "        if word == 'not':\n",
    "            temp_word = 'not_'\n",
    "        elif temp_word == 'not_':\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for s in syn.lemmas():\n",
    "                    for a in s.antonyms():\n",
    "                        antonyms.append(a.name())\n",
    "            if len(antonyms) >= 1:\n",
    "                word = antonyms[0]\n",
    "            else:\n",
    "                word = temp_word + word # when antonym is not found, it will\n",
    "                                    # remain not_happy\n",
    "            \n",
    "            temp_word = ''\n",
    "        if word != 'not':\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "    \n",
    "text = convert_to_antonym(text)\n",
    "print(text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Dealing With Base and Derived Words <a class=\"kk\" id=\"2.5\"></a>\n",
    "\n",
    "Words can be classified into Base word and Derived word. For example 'go' is a base word and  'going', 'gone' and 'went' are its derived words. During data cleaning phase derived word shall be converted to their base counterparts. There are two ways of finding base word of any word - Stemming and Lemmatization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Stemming  <a class=\"kk\" id=\"2.5.1\"></a>\n",
    "\n",
    "Stemming is a rule base technique. In Stemming Base word is identified by chopping the word at end. For instance 'going' and 'gone' will get converted to 'go' but 'went' will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david want to go with alfa but alfa went with charli so david is go with bravo\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "text = \" David wanted to go with Alfa but Alfa went with Charli so David is going with Bravo\"\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "text = stem_words(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Lemmatization   <a class=\"kk\" id=\"2.5.2\"></a>\n",
    "Lemmatization is a dictionary based technique and more accurate than stemming. It looks up to the dictionary to fetch Base word (called as lemma). The obvious downside is, it is slow in processing because has to store and look up the dictionary.\n",
    "\n",
    "Additionally, lemmatization requires Parts of speech tagging. Lets understand POS tagging first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Parts of Speech (POS) tagging \n",
    "\n",
    "Parts of Speech Tagger processes a sequence of words and attaches a part of speech tag to each word. 'nltks poc_tag' library is used for POS tagging. Some POS tags examples are VBZ -> Verb, NN-> Noun, PRP -> preposition, IN -> Interjection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('very', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('observation', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('you.', 'NN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"This is very good observation by you.\"\n",
    "nltk.pos_tag(text.split()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know what exactly each tag specify use nltk help function as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/anastasiya.kuzmich/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('DT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now coming back to Lemmatization,we need to define a wordnet_map (as in below code) and specify for which all parts of speech we need to find Base words otherwise by default it will fetch base words for nouns only.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'David want to go with Alfa but Alfa go with Charli so David be go with Bravo'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV }\n",
    "\n",
    "# without wordnet map it takes evey word as noun\n",
    "text = \"David wanted to go with Alfa but Alfa went with Charli so David is going with Bravo \"\n",
    " \n",
    "def lemma_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word ,wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "\n",
    "lemma_words(text) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Extract Text Using BeautifulSoup <a class=\"kk\" id=\"2.6\"></a>\n",
    "Beautiful Soup is a very useful Python library used for extracting text data from HTML and XML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html><head><title>The NLP story</title></head>\n",
      "<body>\n",
      "<p class=\"title\"><b>The NLP story</b></p>\n",
      "<p class=\"story\">Once upon a time there were three little  techniques; and their names were\n",
      "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
      "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
      "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
      "and they lived till the next conference.</p>\n",
      "<p class=\"story\">...</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "<html><head><title>The NLP story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The NLP story</b></p>\n",
    "<p class=\"story\">Once upon a time there were three little  techniques; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived till the next conference.</p>\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The NLP story\n",
      "\n",
      "The NLP story\n",
      "Once upon a time there were three little  techniques; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived till the next conference.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "text = BeautifulSoup(text, \"html\").text# for HTML decoding\n",
    "# for lxml  decoding use as below\n",
    "#text = BeautifulSoup(text, \"lxml\").text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we know possible ways to clean our text data we will write a clean-up function for our Disaster Tweets dataset.\n",
    "Note that: choice of clean up approaches mainly depend upon problem domain, dataset and individual perception. For instance, for newspaper and journal articles we might not need any spelling correction. Clean-up function is an important factor in deciding overall classification outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "  \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text= re.sub(r'[^\\w\\s#]',' ',text) #Removing every thing other than space, word and hash\n",
    "    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n",
    "    text= re.sub(r'[0-9]',' ',text)\n",
    "    #text = correct_spellings(text)\n",
    "    text = convert_to_antonym(text)\n",
    "    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text to Numeric Conversion <a class=\"kk\" id=\"3\"></a>\n",
    "\n",
    "[Back to Contents](#0.1) \n",
    "\n",
    "Once text is cleaned we need to feed it into learning Algorithm. However, Learning algorithms understand numbers and not words hence, we will convert text into numbers before feeding into any modeling algorithm. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# Image(\"/kaggle/input/images/texttonumber.jpg\",  width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pre Conversion <a class=\"kk\" id=\"3.1\"></a>\n",
    "\n",
    "Before performing text to numeric conversion we will look into few common NLP vocabulary terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Corpus <a class=\"kk\" id=\"3.1.1\"></a>\n",
    "Collection of all available textual data is known as corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= pd.DataFrame(columns=['text'])\n",
    "corpus['text']= pd.concat([train_df[\"text\"], test_df[\"text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Tokenization<a class=\"kk\" id=\"3.1.2\"></a>\n",
    "\n",
    "Text is segmented into its smaller parts called tokens. This segmentation process is known as tokenization. A common example of tokens we can think of is words. However tokens can be something else also such as combination of 2 words. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# Image(\"/kaggle/input/images/tokenization.png\",  width=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Bag of Words (BOW) <a class=\"kk\" id=\"3.1.3\"></a>\n",
    "\n",
    "BOW approach is a way of representing text data. BOW approach recognizes text as just a bag of words and keeps a count of the total occurrences of words in text. It does not consider meaning or context of the word in the document. For example,\n",
    "\n",
    "Text -> \"Now tell me BOW approach is good or not so good?\"\n",
    "\n",
    "BOW -> \"Now\",\"tell\",\"me\",\"BOW\",\"approach\",is\",\"good\",\"or\",\"not\",\"so\",\"good\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 N-Gram <a class=\"kk\" id=\"3.1.4\"></a>\n",
    "\n",
    "N-grams is a contiguous sequence of n words from a given sample of source text. For Example: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# Image(\"/kaggle/input/images/ngram.png\",  width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Conversion <a class=\"kk\" id=\"3.2\"></a>\n",
    "In this section we will look into text to numerical conversion techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 CountVectorization  <a class=\"kk\" id=\"3.2.1\"></a>\n",
    "\n",
    "CountVectorization converts a collection of text documents to a matrix of token counts. It counts the tokens in text.  Total no of tokens i.e. vocabulary size will equal to matrix column length (no of columns) where each column represents a token. A row array represents unique tokens of the record and the number at a cell will indicate count of that token in the record.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Image(\"/kaggle/input/images/countvectorization.png\",  width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "# analyzer should be word/ character\n",
    "#ngram_range lower and upper boundary of the range of n-values\n",
    "\n",
    "## let'sconvert text to vectors\n",
    "train_countvectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "# generating test CountVectorizer matrix\n",
    "test_countvectors = count_vectorizer.transform(test_df[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 21637)\n",
      "  (0, 14003)\t1\n",
      "  (0, 5490)\t1\n",
      "  (0, 2192)\t1\n",
      "  (0, 18669)\t1\n",
      "  (0, 15678)\t1\n",
      "  (0, 13681)\t1\n",
      "  (0, 18777)\t1\n",
      "  (0, 6379)\t1\n",
      "  (0, 12141)\t1\n",
      "  (0, 1852)\t1\n",
      "  (0, 7661)\t1\n",
      "  (0, 19774)\t1\n",
      "  (0, 1851)\t1\n",
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# converting sparce to dense vector \n",
    "print(train_countvectors.shape)\n",
    "print(train_countvectors[0])\n",
    "print(train_countvectors[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 TF-IDF Vectorization <a class=\"kk\" id=\"3.2.2\"></a>\n",
    "\n",
    "TF-IDF stands for 'Term frequency-inverse document frequency'. It measures importance of a token (called as term) with respect to its record (called as document) in a corpus. Every term of a document is assigned a weight after multiplying its term frequency (tf) and inverse document frequency (idf). Internally, different libraries use slightly different formula to calculate 'idf' value though underlying idea remains same.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Image(\"/kaggle/input/images/tfidf.png\",  width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words='english', ngram_range=(1, 1))\n",
    "# training tfidf on corpus\n",
    "tfidf_vectorizer.fit(corpus['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 27641)\n",
      "(3263, 27641)\n",
      "(7613, 27641)\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_tfidfvectors = tfidf_vectorizer.transform(train_df['text'])\n",
    "test_tfidfvectors = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "print(train_tfidfvectors.shape)\n",
    "print(test_tfidfvectors.shape)\n",
    "print(train_tfidfvectors.todense().shape)\n",
    "print(train_tfidfvectors[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Post Conversion <a class=\"kk\" id=\"3.3\"></a>\n",
    "\n",
    "Both Countvectorization and tf-idf technique produce large size matrix. Post text-to-numerical conversion dimensionality reduction techniques can be tried out to reduce matrix size though keeping their relative importance intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 SVD ( TruncatedSVD)   <a class=\"kk\" id=\"3.3.1\"></a>\n",
    "\n",
    "TruncatedSVD transformer performs linear dimensionality reduction by means of truncated Singular Value Decomposition (SVD). It efficiently works on sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 21637)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_countvectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD   \n",
    "tsv = TruncatedSVD(n_components=100)\n",
    "train_countvectors_svd = tsv.fit_transform(train_countvectors) \n",
    "train_tfidfvectors_svd = tsv.fit_transform(train_tfidfvectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 100)\n",
      "(7613, 100)\n"
     ]
    }
   ],
   "source": [
    "print(train_countvectors_svd.shape)\n",
    "print(train_tfidfvectors_svd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML Modeling <a class=\"kk\" id=\"4\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "As our text data is in numerical form, it is ready to feed into ML Algorithm. Now we will train classifier models on our numerical data. Purpose here is to depict basic model performance and not to obtain high score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# base function to train and compare various models performance\n",
    "from sklearn import model_selection\n",
    "def text_modeling( model):\n",
    "    print( \"Model :\"+ str(model))\n",
    "    print('***** F1 Scores *******')\n",
    "    scores=model_selection.cross_val_score(model, train_countvectors.toarray(), train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "    print(\"CountVectorized dataset :\"+str(scores.mean()))\n",
    "    scores= model_selection.cross_val_score(model, train_tfidfvectors.toarray(), train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "    print(\"TF-IDF Vectorized dataset :\"+str(scores.mean()))\n",
    "    scores = model_selection.cross_val_score(model, train_tfidfvectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "    print(\"TF-IDF Vectorized + SVD dataset \"+str(scores.mean()))\n",
    "    scores = model_selection.cross_val_score(model, train_countvectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "    print(\"CountVectorized + SVD dataset \"+str(scores.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Naive Bayes Classifer <a class=\"kk\" id=\"4.1\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Gaussian Classifier <a class=\"kk\" id=\"4.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :GaussianNB()\n",
      "***** F1 Scores *******\n",
      "CountVectorized dataset :0.6060410425793524\n",
      "TF-IDF Vectorized dataset :0.5893905433391442\n",
      "TF-IDF Vectorized + SVD dataset 0.45195187060882497\n",
      "CountVectorized + SVD dataset 0.5304947819453002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gn = GaussianNB()\n",
    "text_modeling(gn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Bernoulli  Classifier <a class=\"kk\" id=\"4.1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :BernoulliNB()\n",
      "***** F1 Scores *******\n",
      "CountVectorized dataset :0.6748069579702234\n",
      "TF-IDF Vectorized dataset :0.6187771501248447\n",
      "TF-IDF Vectorized + SVD dataset 0.5855567647278508\n",
      "CountVectorized + SVD dataset 0.5368897072877494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "#Create a Gaussian Classifier\n",
    " \n",
    "br = BernoulliNB()\n",
    "text_modeling( br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Logistic Regression <a class=\"kk\" id=\"4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :LogisticRegression(solver='liblinear')\n",
      "***** F1 Scores *******\n",
      "CountVectorized dataset :0.6462951211992708\n",
      "TF-IDF Vectorized dataset :0.569332520580874\n",
      "TF-IDF Vectorized + SVD dataset 0.5823671951786524\n",
      "CountVectorized + SVD dataset 0.6279080005581391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "text_modeling( lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Support Vector Machine <a class=\"kk\" id=\"4.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6142621592585806"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "# text_modeling(svm)\n",
    "model_selection.cross_val_score(svm, train_tfidfvectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 XGBoost <a class=\"kk\" id=\"4.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6278393332941637"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "# text_modeling(xgb_clf)\n",
    "model_selection.cross_val_score(xgb_clf, train_tfidfvectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading. In this part we saw various ways to clean text data and convert text into numerical representations so that text shall be processed by Machine learning algorithms.  \n",
    "\n",
    "\n",
    "In the next [part](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-2-dl-perspective) we will see how to take care of synonyms and language specific aspects while doing numeric conversion.We will learn about one of the important concepts of NLP that is 'Word Embeddings' and see how Deep Learning has simplified NLP. \n",
    "\n",
    "If you found this notebook usefull Please Upvote!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Jan 19 2023, 16:36:00) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd44928fdf7d6d27586ebe215c3fe892fbdd87538a3da89a719f39063f6fce1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
